{
	"jobConfig": {
		"name": "Modelo",
		"description": "",
		"role": "arn:aws:iam::096941180198:role/jmmarinq",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Modelo.py",
		"scriptLocation": "s3://aws-glue-assets-096941180198-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-05-20T22:28:30.793Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-096941180198-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-096941180198-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.ml import Pipeline\r\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\r\nfrom pyspark.ml.classification import LogisticRegression\r\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\r\nfrom datetime import datetime\r\n#Inicio de Contexto\r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\nspark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"1\")\r\nspark.conf.set(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory\")\r\nlogger = glueContext.get_logger()\r\n#Configuracion\r\ninput_path = \"s3://nequijmmarinq/processed/clean_data/\"\r\nmodel_output_path = \"s3://nequijmmarinq/models/msg-classifier/\"\r\ntest_path = \"s3://nequijmmarinq/test/\"\r\nversion = \"v_1.0\"\r\ntrained = datetime.utcnow().isoformat()\r\n#Leer los datos limpios \r\nlogger.info(\"Leyendo datos limpios desde S3...\")\r\ndf = spark.read.parquet(input_path)\r\ndf.schema\r\n#Mismas cantidad de los datos del job anterior\r\ndf.count()\r\ndf.show()\r\n#Target\r\ndf = df.filter(F.col(\"inbound\").isNotNull())\r\n#Tokenizador, convertir palabras en vectores\r\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\r\nwordsData = tokenizer.transform(df)\r\nwordsData.show()\r\n#Hashing, proceso de convertir un vector en longitud fija\r\ntf = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=10000)\r\nhashingTF_model = tf.transform(wordsData)\r\nhashingTF_model.show()\r\n#Calculo de frecuencias en las palabras\r\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\").fit(hashingTF_model)\r\ntfidf = idf.transform(hashingTF_model)\r\ntfidf.show()\r\ntfidf.select(\"inbound\", \"rawFeatures\",\"features\").show(truncate=False)\r\n#Estas relaciones son las que entran al modelo, al pipeline \r\n#Se propone una regresion logistica \r\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"inbound\", maxIter=20)\r\n#Ahora con todos esos componentes se propone el pipeline completo \r\npipeline = Pipeline(stages=[tokenizer, tf, idf, lr])\r\n#Dividir el dataset en un 80% train y 20% para la evaluaci√≥n \r\ntrain_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\r\n#Entrenamiento \r\nlogger.info(\"Entrenando modelo...\")\r\nmodel = pipeline.fit(train_df)\r\nlogger.info(\"Evaluando modelo...\")\r\npredictions = model.transform(test_df)\r\nevaluator = BinaryClassificationEvaluator(labelCol=\"inbound\", rawPredictionCol=\"rawPrediction\")\r\nauc = evaluator.evaluate(predictions)\r\nlogger.info(f\"AUC en test set: {auc}\")\r\n#logger.info(\"Guardando modelo entrenado en S3...\")\r\nmodel.write().overwrite().save(model_output_path + f\"{version} +_+{trained}\")\r\n#Guardar el test para la inferencia\r\ntest_df.write.mode(\"overwrite\").parquet(test_path+f\"{version} +_+{trained}\")\r\n"
}